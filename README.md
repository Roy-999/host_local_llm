Figuring ways to host and use local LLMs with vLLM inference engine
